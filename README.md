# Adversarial Examples Attacks and Defenses
Final Project for attacking and defending Image recognition SOTA deep learning models. This is a working repository with implementation of several adversarial examples attack algorithms and defenses. I will be working on improving the code from usability prespective and will add descrition with image samples from my experiments in a few days. Also I plan to release the trained models for both attack and defenses in coming days.
## Disclaimer: This is a personal repository which is a work in progress.
Attacks Implimented:
1. Fast Gradient Sign Method (FGSM)
2. Basic Iterative Method (BIM)
3. Iterative Least-Likely Class (ILLC)
4. DeepFool
5. Carlini-Wagner (CW) L2
6. Carlini-Wagner (CW) Linf

Datasets:
1. cifar10
2. cifar100
3. ImageNet (validation set)

Defenses:
1. Adversarial retraining

** Working on more defense mechanisms **


References:
1. Goodfellow, Ian J, 2014, Explaining and Harnessing Adversarial Examples
2. Kurakin, Alexey, 2016, Adversarial examples in the physical world
3. Moosavi-Dezfooli, Seyed-Mohsen, 2016, Deepfool: a simple and accurate method to fool deep neural networks
4. Carlini, Nicholas, 2017, Towards evaluating the robustness of neural networks


